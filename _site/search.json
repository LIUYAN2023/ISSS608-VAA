[
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "The country of Oceanus has requested the assistance of FishEye International in identifying companies that may be involved in illegal, unreported, and unregulated (IUU) fishing. As part of this collaboration, FishEye’s analysts have been provided with import/export data related to Oceanus’ marine and fishing industries.\nTo facilitate their analysis, FishEye has transformed the trade data into a knowledge graph. During their analysis, FishEye analysts have found that node-link diagrams provide a valuable high-level overview of the knowledge graph. However, they are now seeking visualizations that offer more detailed insights into patterns involving entities within the knowledge graph.\n\nBased on FishEye’s previous experiences, they are aware that companies engaged in illegal fishing tend to cease their operations temporarily but often resume their activities under different names.\nThe objective of this exercise is to compare the activities of these companies over time and identify any indications of their potential return to illicit practices.\nThis exercise source is from: Mini-Challenge 2 task 1"
  },
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#install-r-packages-and-import-dataset",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#install-r-packages-and-import-dataset",
    "title": "Take-home_Ex02",
    "section": "2.1 Install R Packages and Import Dataset",
    "text": "2.1 Install R Packages and Import Dataset\nThe code chunk below uses pacman::p_load() to check if packages are installed. If they are, they will be launched into R. The packages installed are:\n\n\nshow the code\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, lubridate, tidyverse,readxl,igraph)\n\n\njsonlite: A simple and robust JSON parser and generator for R.\ntidygraph: this package provides a tidy API for graph/network manipulation.\nggraph: ggiraph is a tool that allows you to create dynamic ggplot graphs.\nvisNetwork: an R package for network visualization, using vis.js javascript library.\nlubridate: an R package that makes it easier to work with dates and times.\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nreadxl：An R package makes it easy to get data out of Excel and into R.\nigraph:igraph is a fast and open source library for the analysis of graphs or networks."
  },
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#data-introduction",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#data-introduction",
    "title": "Take-home_Ex02",
    "section": "2.2 Data Introduction",
    "text": "2.2 Data Introduction\nIn the code chunk below, fromJSON() of jsonlite package is used to import mc2_challenge_graph.json into R environment.\n\n\nshow the code\nmc2_data <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\nFor the purpose of this study, a directed multi-graph is provided. There are totally 34552 nodes and 5464092 directed edges.\nNode Attributes:\nid – Name of the company that originated (or received) the shipment.\nshpcountry – Country the company most often associated with when shipping.\nrcvcountry – Country the company most often associated with when receiving.\ndataset – Always ‘MC2’.\nEdge Attributes:\narrivaldate – Date the shipment arrived at port in YYYY-MM-DD format.\nhscode – Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu– Customs-declared value of the total shipment, in Oceanus Monetary Units (OMU).\nvolumeteu– The volume of the shipment in ‘Twenty-foot equivalent units’, roughly how many 20-foot standard containers would be required. (Actual number of containers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg– The weight of the shipment in kilograms (if known).\ndataset– Always ‘MC2’.\ntype– Always ‘shipment’ for MC2.\ngenerated_by – Name of the program that generated the edge. (Only found on ‘bundle’ records.)\nNote: Some data provided by Oceanus was anonymized leading to some shipper and receiver names/countries being omitted. These are represented by numerical names in the graph."
  },
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#data-wrangling",
    "title": "Take-home_Ex02",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n2.3.1 Extract Nodes & Edges\nExtracting nodes : involves creating a tibble data frame called mc2_nodes from the mc2_data list object by using the select() function, which serves two purposes: selecting the required fields and re-organise the sequence of the fields.\n\n\nshow the code\nmc2_nodes <- as_tibble(mc2_data$nodes) %>%\n  select(id, shpcountry, rcvcountry)\n\n\n\n\n\n\n\n\nNote\n\n\n\nselect() is used not only to select the field needed but also to re-organise the sequent of the fields.\n\n\nExtracting Edges: The code chunk is used to extract edges data table from mc2_data list object and save the output in a tibble data frame object called mc2_edges.\n\n\nshow the code\nmc2_edges <- as_tibble(mc2_data$links) %>%\n  mutate(ArrivalDate = ymd(arrivaldate)) %>%\n  mutate(Year = year(ArrivalDate)) %>%\n  select(source, target, ArrivalDate, Year, hscode, valueofgoods_omu, \n         volumeteu, weightkg, valueofgoodsusd) %>% \n  distinct()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nmutate() is used two times to create two derive fields.\n\nymd() of lubridate package is used to covert arrivaldate field from character data type into date data type.\nyear() of lubridate package is used to convert the values in ArrivalDate field into year values.\n\nselect() is used not only to select the field needed but also to re-organise the sequent of the fields.\n\n\n\n\n\n2.3.2 Data Preparation _ HScode\nThe Harmonized System is a standardized numerical method of classifying traded products. It is used by customs authorities around the world to identify products when assessing duties and taxes and for gathering statistics.\nIn the original dataframe, there are numerous HScodes that are unrelated to “Fishing”. To filter out observations specifically related to fishing, we utilize the grep function to select only those HScodes that begin with 301-308.\n\n\nshow the code\n# filters the dataframe mc2_edges hscode start with 301-308\nmc2_edges <- mc2_edges[grep(\"^30[1-8]\", mc2_edges$hscode), ]\n\n# Count each hscode frequency\nhscode_counts <- table(mc2_edges$hscode)\n\n# Sort by count from highest to lowest\nhscode_counts_sorted <- sort(hscode_counts, decreasing = TRUE)\n\n# Create a dataframe containing hscode and count\nhscode_counts_df <- data.frame(hscode = names(hscode_counts_sorted), count = hscode_counts_sorted, stringsAsFactors = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngrep(pattern, x) is a function that searches for a specified pattern (pattern) within a character vector (x).\ntable(x) is a function that creates a frequency table of the elements in the vector x. It counts the occurrences of each unique value and returns a table with the counts.\n\n-sort(x, decreasing = FALSE)is a function that sorts the elements in the vector x in ascending order.\n-data.frame(..., stringsAsFactors = FALSE) is a function that creates a data frame from the input arguments provided.\n\n\nAfter the compilation of the hscode_counts_df, it is important to determine the meaning of the HScode. Therefore, we proceed by reading the HScode list and employing the left_join function to establish a linkage between the item description and the hscode_counts_df data frame.\n\n\nshow the code\n# Remove the last zero in hscode\nhscode_counts_df$hscode <- sub(\"0$\", \"\", hscode_counts_df$hscode)\n\n# R an Excel file \"HScode\"\nHScode_list <- read_excel(\"data/HScode.xlsx\", sheet = 1)\n\n# Change the HS Code data type\nHScode_list$`HS Code` <- as.character(HScode_list$`HS Code`)\n\n# hscode_counts_df left join HScode_list to improt the HScode description\nmerged_data <- left_join(hscode_counts_df, select(HScode_list, -S.No.), by = c(\"hscode\" = \"HS Code\"))\n\n\n\n\n\n\n\n\nNote\n\n\n\nsub(pattern, replacement, x) is a function that replaces the first occurrence of a pattern (pattern) in a character vector (x) with a specified replacement string (replacement).\n`read_excel(file, sheet = 1)’ is a function that reads an Excel file (file) and imports the data from the specified sheet (sheet) into R.\n`left_join(x, y, by = c(“column_x” = “column_y”))’ is a function that performs a left join operation between two data frames (x and y) based on the specified column(s) to match (by).\n\n\nWe have summarized the top 6 most frequently occurring HScodes in the dataframe, along with their corresponding fish types based on the HScode descriptions. These HScodes and their associated fish types will serve as the starting point for our research.\n\nTop Frequency HScode\n\n\nHScode\nCount\nFish Type\n\n\n\n\n306170\n156204\nOther shrimps and prawns\n\n\n304620\n87340\nCatfish\n\n\n304610\n36615\nTilapias\n\n\n304710\n27231\nCod\n\n\n304750\n23926\nAlaska Pollack\n\n\n304810\n21433\nPacific salmonAtlantic salmon and Danube salmon\n\n\n\nThe HScode serves as an indicator of the product variety within each company. To analyze this, we have introduced a new variable called “HScode_type” that tallies the number of unique HScode values found in all transaction records for each company (node).\n\n\nshow the code\n# Create transitional dataframe to record all the company and corresponding Year\ntransition_data_HScode <- data.frame(\n  company = c(mc2_edges$source, mc2_edges$target),\n  HScode = c(mc2_edges$hscode, mc2_edges$ hscode)\n)\n\n# Aggregating companies & count each company's running year and running_ year_ Category\naggregate_data_HScode <- transition_data_HScode %>%\n  group_by(company) %>%\n  mutate(HScode_type = n_distinct(HScode)) %>%\n  distinct(company, HScode_type)%>%\n  mutate(HScode_category = cut(HScode_type, breaks = c(-Inf, 1, Inf), labels = c(\"<=1\",\">1\")))\n  \n# Filter the data \nfiltered_data <- subset(aggregate_data_HScode, HScode_type < 10)\n\nggplot(filtered_data, aes(x = HScode_type)) +\n  geom_bar(fill = '#808de8') +\n  labs(x = \"Running Year\", y = \"Count\") +\n  ggtitle(\"Bar Chart of HScode_type (<10)\") +\n  scale_x_continuous(breaks = 1:9, labels = 1:9) +\n  theme(axis.title.y = element_text(angle = 0),\n        axis.ticks.x = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = 'bisque3'),\n        plot.subtitle = element_text(color = \"dimgrey\", size = 12, face = \"bold\", hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nc()is used to concatenate or combine multiple values or objects into a single vector:\n\nc(mc2_edges\\(source, mc2_edges\\)target) creates a vector that concatenates the values of the source column and the target column from the mc2_edges data frame. This vector contains all the company names.\nc(mc2_edges\\(hscode, mc2_edges\\)hscode) creates a vector that concatenates the values of the hscode column from the mc2_edges data frame. This vector contains all the corresponding hscode for each company.\n\ngroup_by() is used to aggregate values by company. mutate() creates a new column called HScode_type, which contains the number of distinct HScode values for each company.distinct() keeps only the distinct combinations of company and HScode_type in the transition_data_HScode data frame. cut()categorizes the HScode_type values into two categories: “<=1” and “>1” based on the breakpoints.\nsubset(x, subset) is a function used to subset a data frame (x) based on specified conditions (subset).\n\n\n\nIn order to gain a holistic understanding of the distribution of companies with different product varieties, we have generated a histogram depicting the frequency of HScode_type values below 10. Upon examining the histogram, we observe a substantial proportion of companies having an HScode_type value of 1 compared to other HScode_type values.\nTo further categorize the companies based on their product variety, we have introduced a new categorical variable named “HScode_category.” This variable assigns a label to each node, differentiating between HScode_type values less than or equal to 1 and those greater than 1. HScode_type <= 1 indicate companies with a narrow focus, specializing in a specific fish type. Conversely, HScode_type >1 indicates companies with a relatively diverse range of products.\n\n\n2.3.3 Data Preparation _ Running Year Category\nIn light of the circumstance where companies engaged in illegal fishing activities might cease operations but subsequently resume their activities under a different corporate identity, it is noteworthy that companies with shorter running years possess a higher likelihood of engaging in illicit behaviors.so we create transition_data to count each company’s running years.\n\n\nshow the code\n# Create transitional dataframe to record all the company and corresponding Year\ntransition_data_Year <- data.frame(\n  company = c(mc2_edges$source, mc2_edges$target),\n  Year = c(mc2_edges$Year, mc2_edges$Year)\n)\n\n# Aggregating companies & count each company's running year and running_ year_ Category\naggregate_data_Year <- transition_data_Year %>%\n  group_by(company) %>%\n  mutate(running_year = n_distinct(Year)) %>%\n  distinct(company, running_year) %>%\n  mutate(running_year_category = cut(running_year, breaks = c(-Inf, 2, Inf), labels = c(\"<=2\",\">2\")))\n\n\nTo facilitate analysis, we have categorized the running years into distinct intervals, namely “<=2” and “>2”, to assess the stability of the companies. Within these intervals:\n\n“<=2” category represents companies with lower stability\n“>2” years category signifies a relatively high level of stability.\n\nIt is worth noting that companies with lower stability, falling within the “<=2” category, tend to exhibit a higher likelihood of being associated with suspected illegal activities.\n\n\nshow the code\n# Create a barchart of running_year distribution with adjusted x-axis labels\nggplot(aggregate_data_Year, aes(x = running_year)) +\n  geom_bar(fill = '#808de8') +\n  labs(x = \"Running Year\", y = \"Count\") +\n  ggtitle(\"Distribution of Running Year\") +\n  scale_x_continuous(breaks = 1:7, labels = 1:7) +\n  theme(axis.title.y = element_text(angle = 0),\n        axis.ticks.x = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color = 'bisque3'),\n        plot.subtitle = element_text(color = \"dimgrey\", size = 12, face = \"bold\", hjust = 0.5))\n\n\n\n\n\nThe barchart depicting the “Distribution of Running Year” reveals a notable observation:\n\nA considerable proportion of companies, exceeding 4000 in number, have a running year of merely one year.\nAs the running year increases, there is a gradual decline in the number of companies.\nHowever, it is intriguing to note that there exists a substantial count of over 1000 companies that have been operating for seven years, surpassing the counts of companies with three, four, five, and six running years.\n\n\n\n2.3.4 Data Preparation _ Betweenness/In-degree/Out-degree\nCentrality is a significant characteristic in network analysis. In this section, we will compute three important centrality measures for each node: betweenness centrality, in-degree centrality, and out-degree centrality.\nBetweenness centrality: This measure quantifies the extent to which a node acts as a bridge or mediator in the network. It calculates the fraction of shortest paths between all pairs of nodes that pass through a particular node.\nIn-degree centrality: This measure focuses on the number of incoming edges or connections to a node. It signifies the degree to which a node receives interactions or dependencies from other nodes in the network.\nOut-degree centrality: This measure pertains to the number of outgoing edges or connections from a node. It indicates the degree to which a node initiates interactions or dependencies with other nodes in the network.\nThrough the computation of these centrality measures, we can gain insights into the structural importance and influence of each node within the network, enabling us to better understand the dynamics and functioning of the network as a whole.\n\n\nshow the code\n# Aggregating edges data\nmc2_all_edges_aggregated <- mc2_edges %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngroup_by() is used to aggregate values by source, target, hscode, Year.\nsummarise() and n() are used to count the aggregated records.\nfilter() is then used to perform two selections to select all records whereby source are not equal to target to select all records whereby the values of their weights field are greater than 20\n\n\n\n\n\nshow the code\n# Extracting unique node IDs\nid1_all <- mc2_all_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2_all <- mc2_all_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_all_nodes_extracted <- rbind(id1_all, id2_all) %>%\n  distinct()\n\nmc2_all_graph <- tbl_graph(nodes = mc2_all_nodes_extracted,\n                       edges = mc2_all_edges_aggregated,\n                       directed = TRUE)\n\n\n# Calculate betweenness centrality\nbetweenness_centrality <- betweenness(mc2_all_graph)\n\n# Calculate in-degree centrality\nin_degree_centrality <- degree(mc2_all_graph, mode = \"in\")\n\n# Calculate out-degree centrality\nout_degree_centrality <- degree(mc2_all_graph, mode = \"out\")\n\n# Convert the node attributes and ID to a dataframe\nnode_data <- data.frame(\n  ID = V(mc2_all_graph)$id,\n  Betweenness = betweenness_centrality,\n  In_Degree = in_degree_centrality,\n  Out_Degree = out_degree_centrality\n)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nrbind()combines the id1_all and id2_all data frames vertically, we get a single data frame mc2_all_nodes_extracted with all the distinct node IDs.\nbetweenness()calculates the betweenness centrality for each node.\ndegree(, mode = \"in\")calculates the in-degree centrality for each node.\ndegree(, mode = \"out\")calculates the out-degree centrality for each node.\n\n\n\nBased on the node centrality table, we can classify the nodes into three distinct groups: Wholesaler: Nodes exhibiting non-zero values in betweenness centrality, in-degree centrality, and out-degree centrality. These nodes demonstrate a comprehensive pattern of connectivity within the network, indicating their involvement in multiple fish trading flows. We can infer that these nodes operate as wholesalers, facilitating the buying and selling of fish products. Buyer: Nodes displaying non-zero values in in-degree centrality, while both betweenness centrality and out-degree centrality possess zero values. These nodes primarily act as recipients or buyers within the network, buying fish products from other nodes. Supplier: Nodes featuring non-zero values in out-degree centrality, while both betweenness centrality and in-degree centrality exhibit zero values. These nodes operate as suppliers within the network, selling fish product to other nodes.\n\n\nshow the code\nnode_data$company_label <- NA  # Create a new column for storing company_ Label\n\n# Assign values to company_label based on conditions\nnode_data$company_label[node_data$Betweenness != 0 & node_data$Betweenness > 20] <- \"wholesaler_high\"\nnode_data$company_label[node_data$Betweenness != 0 & node_data$Betweenness <= 20 & node_data$Betweenness >= 10] <- \"wholesaler_medium\"\nnode_data$company_label[node_data$Betweenness != 0 & node_data$Betweenness < 10] <- \"wholesaler_low\"\n\nnode_data$company_label[node_data$Betweenness == 0 & node_data$Out_Degree == 0 & node_data$In_Degree != 0 & node_data$In_Degree > 150] <- \"buyer_high\"\nnode_data$company_label[node_data$Betweenness == 0 & node_data$Out_Degree == 0 & node_data$In_Degree != 0 & node_data$In_Degree <= 150 & node_data$In_Degree >= 50] <- \"buyer_mediu m\"\nnode_data$company_label[node_data$Betweenness == 0 & node_data$Out_Degree == 0 & node_data$In_Degree != 0 & node_data$In_Degree < 50] <- \"buyer_low\"\n\nnode_data$company_label[node_data$Betweenness == 0 & node_data$In_Degree == 0 & node_data$Out_Degree != 0 & node_data$Out_Degree > 50] <- \"supplier_high\"\nnode_data$company_label[node_data$Betweenness == 0 & node_data$In_Degree == 0 & node_data$Out_Degree != 0 & node_data$Out_Degree <= 50 & node_data$Out_Degree >= 20] <- \"supplier_medium\"\nnode_data$company_label[node_data$Betweenness == 0 & node_data$In_Degree == 0 & node_data$Out_Degree != 0 & node_data$Out_Degree < 20] <- \"supplier_low\"\n\n\n# Calculate the normalization of Betweenness, In_Degree, and Out_Degree\nnormalized_betweenness <- (node_data$Betweenness - min(node_data$Betweenness)) / (max(node_data$Betweenness) - min(node_data$Betweenness))\nnormalized_in_degree <- (node_data$In_Degree - min(node_data$In_Degree)) / (max(node_data$In_Degree) - min(node_data$In_Degree))\nnormalized_out_degree <- (node_data$Out_Degree - min(node_data$Out_Degree)) / (max(node_data$Out_Degree) - min(node_data$Out_Degree))\n\n# Add the 'size' column based on conditions\nnode_data$size_all <- ifelse(node_data$Betweenness != 0, normalized_betweenness,\n                         ifelse(node_data$Out_Degree != 0, normalized_out_degree,\n                                ifelse(node_data$In_Degree != 0, normalized_in_degree, NA)))\n\n# Add the 'size' column based on conditions\nnode_data$company_category <- ifelse(node_data$Betweenness != 0, \"Wholesaler\",\n                         ifelse(node_data$Out_Degree != 0, \"Supplier\",\n                                ifelse(node_data$In_Degree != 0,\"Buyer\", NA)))\n\n\nTo comprehend the range of centrality within each model, we further divide the nodes into high, medium, and low levels based on their centrality scores, and subsequently generate box plots to visualize the distribution of centrality scores within each group.\n\n\nshow the code\nlibrary(ggplot2)\n\n# Create a custom color vector\ncolors <- c(\"#FF6F61\", \"#6B5B95\", \"#88B04B\", \"#F7CAC9\", \"#92A8D1\", \"#955251\", \"#B565A7\", \"#009B77\", \"#DD4124\")\n\n# Extract the required data\nboxplot_data <- node_data %>%\n  select(company_label, Betweenness, In_Degree, Out_Degree) %>%\n  pivot_longer(cols = -company_label, names_to = \"variable\", values_to = \"value\")\n\n# Plot the boxplot\nboxplot_plot <- ggplot(boxplot_data, aes(x = company_label, y = value, fill = company_label)) +\n  geom_boxplot() +\n  facet_wrap(~ variable, scales = \"free_y\", ncol = 1) +\n  theme_minimal() +\n  labs(x = \"company_label\", y = \"\") +\n  coord_flip() +\n  scale_fill_manual(values = colors)\n\nboxplot_plot\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n-select()selects specific columns (company_label, Betweenness, In_Degree, Out_Degree) from the node_data data frame.\n\npivot_longer()reshapes the data frame from a wide format to a long format. It takes all columns except for company_label and pivots them into two new columns: variable and value.\n\n\n\nFrom the boxplot analysis, the following observations can be made:\n- Only nodes categorized as wholesalers exhibit non-zero values in betweenness centrality. The range of betweenness centrality scores for wholesalers falls between 24 and 51.\n- Nodes categorized as suppliers have a zero in-degree centrality. Nodes categorized as buyers have a higher in-degree centrality compared to wholesalers within the same level. For instance, the in-degree centrality range for high-level buyers is from 192 to 380, whereas the in-degree centrality range for high-level wholesalers is from 3 to 111.\n- Nodes categorized as buyers have a zero in-degree centrality. Nodes categorized as suppliers have a higher out-degree centrality compared to wholesalers within the same level. For example, the out-degree centrality range for high-level suppliers is from 53 to 94, whereas the out-degree centrality range for high-level wholesalers is from 1 to 39.\nFor the subsequent analysis, we will simplify the node categorization into three basic models: wholesalers, buyers, and suppliers, in order to avoid excessive grouping and maintain focus on the fundamental classification.\n\n\n2.3.5 Data Preparation _ Data Feature Extraction\nIn this section, we will create a new data frame called “node_data_feature” to consolidate the features obtained from previous analyses. This data frame will incorporate information regarding the nodes’ running_year_category, HScode_category, and company_label (wholesaler, supplier, or buyer), thereby enabling further subgrouping of the nodes based on these features.\n\n\nshow the code\nnode_data_feature <- node_data %>%\n  left_join(aggregate_data_HScode %>% distinct(company, HScode_category), by = c(\"ID\" = \"company\"))  %>%\n  left_join(aggregate_data_Year %>% distinct(company, running_year_category), by = c(\"ID\" = \"company\")) \n\n\n\n\n\n\n\n\nNote\n\n\n\nBy two left_join(),node_data_feature includes the original columns from the node_data data frame, as well as the additional columns “HScode_category” and “running_year_category”.\n\n\n\n\nshow the code\n# Create Company_group column and assign values based on conditions\nnode_data_feature$Company_group <- ifelse(\n  node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Supplier\",\n  \"Unstable_Uniform_Supplier\",\n  ifelse(\n    node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Supplier\",\n    \"Unstable_Diverse_Supplier\",\n    ifelse(\n      node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Supplier\",\n      \"Stable_Uniform_Supplier\",\n      ifelse(\n        node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Supplier\",\n        \"Stable_Diverse_Supplier\",\n        ifelse(\n          node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Buyer\",\n          \"Unstable_Uniform_Buyer\",\n          ifelse(\n            node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Buyer\",\n            \"Unstable_Diverse_Buyer\",\n            ifelse(\n              node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Buyer\",\n              \"Stable_Uniform_Buyer\",\n              ifelse(\n                node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Buyer\",\n                \"Stable_Diverse_Buyer\",\n                ifelse(\n                  node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Wholesaler\",\n                  \"Unstable_Uniform_Wholesaler\",\n                  ifelse(\n                    node_data_feature$running_year_category == \"<=2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Wholesaler\",\n                    \"Unstable_Diverse_Wholesaler\",\n                    ifelse(\n                      node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \"<=1\" & node_data_feature$company_category == \"Wholesaler\",\n                      \"Stable_Uniform_Wholesaler\",\n                      ifelse(\n                        node_data_feature$running_year_category == \">2\" & node_data_feature$HScode_category == \">1\" & node_data_feature$company_category == \"Wholesaler\",\n                        \"Stable_Diverse_Wholesaler\",\n                        \"Unknown\"  # Handling unmatched situations can be changed according to actual needs\n                      )\n                    )\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n)\n\n\n\n\nshow the code\n# Create a named vector that specifies the color corresponding to each group\ngroup_colors <- c(\"Unstable_Uniform_Supplier\" = \"greenyellow\",\n                  \"Unstable_Diverse_Supplier\" = \"green2\",\n                  \"Stable_Uniform_Supplier\" = \"green4\",\n                  \"Stable_Diverse_Supplier\" = \"khaki\",\n                  \"Unstable_Uniform_Buyer\" = \"yellow3\",\n                  \"Unstable_Diverse_Buyer\" = \"yellow4\",\n                  \"Stable_Uniform_Buyer\" = \"mediumorchid1\",\n                  \"Stable_Diverse_Buyer\" = \"mediumpurple1\",\n                  \"Unstable_Uniform_Wholesaler\" = \"mediumslateblue\",\n                  \"Unstable_Diverse_Wholesaler\" = \"lightpink\",\n                  \"Stable_Uniform_Wholesaler\" = \"lightpink2\",\n                  \"Stable_Diverse_Wholesaler\" = \"lightpink3\")\n\n# Count Company_ Number of groups per category\ngroup_counts <- table(node_data_feature$Company_group)\n\n# Convert count results into dataframe\ngroup_counts_df <- data.frame(Company_group = names(group_counts), Frequency = as.numeric(group_counts))\n\n# Draw a bar chart for the company group\ngroup_counts_df$Company_group <- reorder(group_counts_df$Company_group, group_counts_df$Frequency)\nggplot(group_counts_df, aes(x = Company_group, y = Frequency, fill = Company_group)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = group_colors) +\n  labs(x = \"Company Group\", y = \"Count\") +\n  ggtitle(\"Count of Company Groups\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n-reorder() is used to reorder the levels of the Company_group factor in the group_counts_df dataframe based on the Frequency variable.\n-coord_flip() is used to flip the x and y axes, resulting in a horizontal bar chart.\n\n\nTo visualize the distribution of companies within these groups, a bar chart is plotted based on the company group. It is evident from the chart that the “Stable_Diverse_Supplier” group exhibits the highest percentage, followed by the “Stable_Diverse_Buyer” group. These two groups demonstrate noticeably higher percentages compared to the other groups, indicating their prominence within the network."
  },
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#interactive-network-graph-with-visnetwork",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#interactive-network-graph-with-visnetwork",
    "title": "Take-home_Ex02",
    "section": "3.1 Interactive Network Graph with visNetwork",
    "text": "3.1 Interactive Network Graph with visNetwork\nIn this section, a network plot is generated for a subset of nodes focusing on those with a specific HScode (306170). To manage the large volume of nodes, a filter condition based on the Year variable is applied, enabling the separation of networks for different years.\nIn the network plot, each node is assigned a color based on its company_group, allowing for a clear visualization of its distinctive features within the network. This coloring scheme facilitates the identification and interpretation of the nodes’ characteristics and facilitates the analysis of their interactions and connections.\n\n2028202920302031203220332034\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2028\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2029\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2030\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2031\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2032\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2033\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2034\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n# Plot the network graph using visNetwork\ng <- visNetwork(nodes = mc2_nodes_extracted, edges = mc2_edges_aggregated,height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size =50,color = group_colors) %>%\n  visEdges(color = list(highlight = \"lightgray\")) %>%\n  visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)\n\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nselectedBy = “group”: Specifies that nodes can be selected by their group attribute. highlightNearest: Configures the highlighting of nearest nodes.\nenabled = TRUE: Enables the highlighting of nearest nodes.\ndegree = 1: Specifies the degree of nearest nodes to be highlighted. Here, it is set to 1, meaning only the immediate neighboring nodes will be highlighted.\nhover = TRUE: Enables highlighting on hover.\nlabelOnly = TRUE: Specifies that only the label of the node will be highlighted.\nnodesIdSelection = TRUE: Allows selecting nodes by their ID."
  },
  {
    "objectID": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#ego-network-for-key-node",
    "href": "Take-home Exercise/Take-home Exercise 2/Take-home_Ex02.html#ego-network-for-key-node",
    "title": "Take-home_Ex02",
    "section": "3.2 Ego-Network for Key Node",
    "text": "3.2 Ego-Network for Key Node\nFor further analysis of key node connections, an ego-net approach is employed to examine the connections of a specific node, namely “hǎi dǎn Corporation Wharf,” which has been identified based on the previous section’s analysis. The selection of this node is driven by its numerous short-running year connections, suggesting potential significance and relevance in the network structure. By focusing on the ego-net of Hai Dan Corporation, we can gain deeper insights into its immediate connections and study its specific role and influence within the network.\n\n2028202920302031203220332034\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2028\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)   \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2029\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2030\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2031\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2032\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2033\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\nshow the code\n# Aggregating and filtering edges data\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(hscode == \"306170\"& Year == \"2034\") %>%\n  group_by(from = source, to = target, hscode, Year) %>%\n  summarise(weights = n()) %>%\n  filter(from != to) %>%\n  filter(weights > 20) %>%\n  ungroup()\n\n# Extracting unique node IDs\nid1 <- mc2_edges_aggregated %>%\n  select(from) %>%\n  rename(id = from)\nid2 <- mc2_edges_aggregated %>%\n  select(to) %>%\n  rename(id = to)\nmc2_nodes_extracted <- rbind(id1, id2) %>%\n  distinct()\n\n# Joining extracted nodes with company data to add running_year_category information\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  left_join(node_data_feature %>% distinct(ID, size_all, Company_group), by = c(\"id\" = \"ID\")) \n\nmc2_nodes_extracted <- mc2_nodes_extracted %>%\n  rename(group = Company_group)\n\n\nego_edges <- mc2_edges_aggregated %>%\n  filter(from == \"hǎi dǎn Corporation Wharf\" | to == \"hǎi dǎn Corporation Wharf\")\n\n# Extract unique node IDs from ego_edges\nego_node_ids <- unique(c(ego_edges$from, ego_edges$to))\n\n# Filter ego_nodes using the unique node IDs\nego_nodes <- mc2_nodes_extracted %>%\n  filter(id %in% ego_node_ids) %>%\n  mutate(node_id = row_number())\n\n# Update ego_edges with the corresponding node IDs\nego_edges <- ego_edges %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"from\" = \"id\")) %>%\n  rename(from_node = node_id) %>%\n  left_join(ego_nodes %>% select(id, node_id), by = c(\"to\" = \"id\")) %>%\n  rename(to_node = node_id)\n\n\nvisNetwork(nodes = ego_nodes, edges = ego_edges, height = \"500px\", width = \"100%\" ) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visNodes(size = 20) %>%\n  visEdges(color = \"gray\") %>%\nvisEdges(color = list(highlight = \"lightgray\")) %>%\n visOptions(selectedBy = \"group\",\n             highlightNearest = list(enabled = TRUE,\n                                     degree = 1,\n                                     hover = TRUE,\n                                     labelOnly = TRUE),\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visInteraction(navigationButtons = TRUE) %>%\n  visLayout(randomSeed = 123)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFiltering the mc2_edges_aggregated dataframe to extract the edges that involve the node “hǎi dǎn Corporation Wharf” as either the source or the target. The resulting subset is assigned to the ego_edges dataframe.\nExtracting the unique node IDs from the ego_edges dataframe.\nFiltering the mc2_nodes_extracted dataframe to select only the nodes whose IDs are present in the ego_node_ids vector. This is done using the %in% operator to match the values of the “id” column with the values in ego_node_ids."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\n\n\n\n\nThe code chunk below uses p_load() of pacman package to check if packages are installed in the computer. If they are, then they will be launched into R. The R packages installed are: plotly and tidyverse\n\n\nshow the code\npacman::p_load('plotly', 'tidyverse')\n\n\n\n\nshow the code\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\n\ncolorspace (2.0-3 -> 2.1-0) [CRAN]\ncli        (3.4.1 -> 3.6.1) [CRAN]\n\n\n\n\nshow the code\nlibrary(ggtern)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\nImport Data\n\n\nshow the code\n#Reading the data into R environment\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nshow the code\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)\n\nagpop_mutated\n\n\n# A tibble: 234 × 25\n   PA         SZ       Year  `AGE0-4` `AGE05-9` `AGE10-14` `AGE15-19` `AGE20-24`\n   <chr>      <chr>    <chr>    <dbl>     <dbl>      <dbl>      <dbl>      <dbl>\n 1 Ang Mo Kio Ang Mo … 2018       180       270        320        300        260\n 2 Ang Mo Kio Cheng S… 2018      1060      1080       1080       1260       1400\n 3 Ang Mo Kio Chong B… 2018       900       900       1030       1220       1380\n 4 Ang Mo Kio Kebun B… 2018       720       850       1010       1120       1230\n 5 Ang Mo Kio Sembawa… 2018       220       310        380        500        550\n 6 Ang Mo Kio Shangri… 2018       550       630        670        780        950\n 7 Ang Mo Kio Tagore   2018       260       340        430        500        640\n 8 Ang Mo Kio Townsvi… 2018       830       930        930        860       1020\n 9 Ang Mo Kio Yio Chu… 2018       160       160        220        260        350\n10 Ang Mo Kio Yio Chu… 2018       810      1070       1300       1450       1500\n# ℹ 224 more rows\n# ℹ 17 more variables: `AGE25-29` <dbl>, `AGE30-34` <dbl>, `AGE35-39` <dbl>,\n#   `AGE40-44` <dbl>, `AGE45-49` <dbl>, `AGE50-54` <dbl>, `AGE55-59` <dbl>,\n#   `AGE60-64` <dbl>, `AGE65-69` <dbl>, `AGE70-74` <dbl>, `AGE75-79` <dbl>,\n#   `AGE80-84` <dbl>, AGE85over <dbl>, YOUNG <dbl>, ACTIVE <dbl>, OLD <dbl>,\n#   TOTAL <dbl>\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n\nshow the code\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\nshow the code\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n\nshow the code\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\") %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-1",
    "title": "Hands-on_Ex06",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\n\nInstall and launching R Packages\nThe code chunk below uses p_load() of pacman package to check if packages are installed in the computer. If they are, then they will be launched into R. The R packages installed are: plotly and tidyverse\nCorrelation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\nTo reveal the relationship between high-dimensional variables pair-wisely. To input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise. As a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable. When the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\nRendering the value of a correlation to depict its sign and magnitude, and Reordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception. In this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R.\n\n\nshow the code\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-1",
    "title": "Hands-on_Ex06",
    "section": "2.2 Data Preparation",
    "text": "2.2 Data Preparation\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\nImport Data\n\n\nshow the code\nwine <- read_csv(\"data/wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-correlation-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-correlation-matrix",
    "title": "Hands-on_Ex06",
    "section": "2.2 Building Correlation Matrix",
    "text": "2.2 Building Correlation Matrix\nBuilding Correlation Matrix: pairs() method. Figure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\n\nshow the code\npairs(wine[,1:11])\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#drawing-the-lower-corner",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#drawing-the-lower-corner",
    "title": "Hands-on_Ex06",
    "section": "2.3 Drawing the lower corner",
    "text": "2.3 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\n\nshow the code\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\n\nshow the code\npairs(wine[,2:12], lower.panel = NULL)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#including-with-correlation-coefficients",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#including-with-correlation-coefficients",
    "title": "Hands-on_Ex06",
    "section": "2.4 Including with correlation coefficients",
    "text": "2.4 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\n\nshow the code\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr <- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr <- abs(cor(x, y, use=\"complete.obs\"))\ntxt <- format(c(r, 0.123456789), digits=digits)[1]\ntxt <- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on_Ex06",
    "section": "2.5 Visualising Correlation Matrix: ggcormat()",
    "text": "2.5 Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\ncorrgram ellipse corrplot On top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\n\nshow the code\n#install.packages(\"ggcorrplot\")\n#library(ggcorrplot)\n#ggstatsplot::ggcorrmat(\n#  data = wine, \n#  cor.vars = 1:11)\n\n\n\n\nshow the code\n#ggstatsplot::ggcorrmat(\n#  data = wine, \n#  cor.vars = 1:11,\n#  ggcorrplot.args = list(outline.color = \"black\", \n#                         hc.order = TRUE,\n#                         tl.cex = 10),\n#  title    = \"Correlogram for wine dataset\",\n#  subtitle = \"Four pairs are no significant at p < 0.05\"\n#)\n\n\nThings to learn from the code chunk above:\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram. ggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits. The sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\n\nshow the code\n#ggplot.component = list(\n#    theme(text=element_text(size=5),\n#      axis.text.x = element_text(size = 8),\n#      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#building-multiple-plots",
    "title": "Hands-on_Ex06",
    "section": "2.6 Building multiple plots",
    "text": "2.6 Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\n\nshow the code\n#grouped_ggcorrmat(\n#  data = wine,\n#  cor.vars = 1:11,\n#  grouping.var = type,\n#  type = \"robust\",\n#  p.adjust.method = \"holm\",\n#  plotgrid.args = list(ncol = 2),\n#  ggcorrplot.args = list(outline.color = \"black\", \n#                         hc.order = TRUE,\n#                         tl.cex = 10),\n#  annotation.args = list(\n#    tag_levels = \"a\",\n#    title = \"Correlogram for wine dataset\",\n#    subtitle = \"The measures are: alcohol, sulphates, fixed #acidity, citric acid, chlorides, residual sugar, density, free #sulfur dioxide and volatile acidity\",\n#    caption = \"Dataset: UCI Machine Learning Repository\"\n#  )\n#)\n\n\nThings to learn from the code chunk above:\nto build a facet plot, the only argument needed is grouping.var. Behind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier. Likewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on_Ex06",
    "section": "2.7 Visualising Correlation Matrix using corrplot Package",
    "text": "2.7 Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\n\nshow the code\nwine.cor <- cor(wine[, 1:11])\n\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\n\nshow the code\ninstall.packages(\"corrplot\")\nlibrary(corrplot)\n\n\n\n\nshow the code\ncorrplot(wine.cor)\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-visual-geometrics",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-visual-geometrics",
    "title": "Hands-on_Ex06",
    "section": "2.8 Working with visual geometrics",
    "text": "2.8 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\n\nshow the code\ncorrplot(wine.cor, \n         method = \"ellipse\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-layout",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-layout",
    "title": "Hands-on_Ex06",
    "section": "2.9 Working with layout",
    "text": "2.9 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\n\nshow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\n\nshow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-mixed-layout",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#working-with-mixed-layout",
    "title": "Hands-on_Ex06",
    "section": "2.10 Working with mixed layout",
    "text": "2.10 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\n\nshow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\n\nshow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-corrgram-with-the-significant-test",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-corrgram-with-the-significant-test",
    "title": "Hands-on_Ex06",
    "section": "2.11 Combining corrgram with the significant test",
    "text": "2.11 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\n\nshow the code\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\n\nshow the code\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#reorder-a-corrgram",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#reorder-a-corrgram",
    "title": "Hands-on_Ex06",
    "section": "2.12 Reorder a corrgram",
    "text": "2.12 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details. “FPC” for the first principal component order. “hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used. “hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”. “alphabet” for alphabetical order. “AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\n\nshow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#reordering-a-correlation-matrix-using-hclust",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#reordering-a-correlation-matrix-using-hclust",
    "title": "Hands-on_Ex06",
    "section": "2.13 Reordering a correlation matrix using hclust",
    "text": "2.13 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n\nshow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-2",
    "title": "Hands-on_Ex06",
    "section": "3.1 Getting Started",
    "text": "3.1 Getting Started\n\nInstall and launching R Packages\nThe code chunk below uses p_load() of pacman package to check if packages are installed in the computer. If they are, then they will be launched into R. The R packages installed are: seriation, dendextend, heatmaply and tidyverse.\n\n\nshow the code\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nshow the code\nwh <- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-2",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-2",
    "title": "Hands-on_Ex06",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\nNext, we need to change the rows by country name instead of row number by using the code chunk below.\n\n\nshow the code\nrow.names(wh) <- wh$Country\n\n\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\n\nshow the code\nwh1 <- dplyr::select(wh, c(3, 7:12))\nwh_matrix <- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#static-heatmap",
    "title": "Hands-on_Ex06",
    "section": "3.3 Static Heatmap",
    "text": "3.3 Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\nheatmap()of R stats package. It draws a simple heatmap. heatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function. pheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps. ComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here. superheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here. In this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\nheatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\n\nshow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n::: call out\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n:::\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\n\nshow the code\nwh_heatmap <- heatmap(wh_matrix)\n\n\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap. Here, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\n\nshow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-interactive-heatmap",
    "title": "Hands-on_Ex06",
    "section": "3.4 Creating Interactive Heatmap",
    "text": "3.4 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\nshow the code\nheatmaply(mtcars)\n\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\n\nshow the code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\nNote that:\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap. The text label of each raw, on the other hand, is placed on the right hand side of the heat map. When the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\nData trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\nScaling method: When all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation. The scale argument in heatmaply() supports column and row scaling.\nThe code chunk below is used to scale variable values columewise.\n\n\nshow the code\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\nNormalising method\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\nPercentising method\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nshow the code\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#clustering-algorithm",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#clustering-algorithm",
    "title": "Hands-on_Ex06",
    "section": "3.5 Clustering algorithm",
    "text": "3.5 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method). hclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust. dist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”. hclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC). In general, a clustering model can be calibrated either manually or statistically.\n\nManual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nStatistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nshow the code\nwh_d <- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nshow the code\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nSeriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nWorking with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nThe finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\nk_row is used to produce 5 groups. margins is used to change the top margin to 60 and row margin to 200. fontsizw_row and fontsize_col are used to change the font size for row and column labels to 4. main is used to write the main title of the plot. xlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nshow the code\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-3",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-3",
    "title": "Hands-on_Ex06",
    "section": "4.1 Getting Started",
    "text": "4.1 Getting Started\n\nInstall and launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\n\nshow the code\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-3",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-3",
    "title": "Hands-on_Ex06",
    "section": "4.2 Data Preparation",
    "text": "4.2 Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\n\nshow the code\nwh <- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on_Ex06",
    "section": "4.3 Plotting Static Parallel Coordinates Plot",
    "text": "4.3 Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\nPlotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\n\nshow the code\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\nPlotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\n\nshow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\nThings to learn from the code chunk above.\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name. scale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one. alphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1. boxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE. title argument is used to provide the parallel coordinates plot a title.\n\n\nParallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\n\nshow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\nRotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\n\nshow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\nThing to learn from the code chunk above:\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\nAdjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\n\nshow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on_Ex06",
    "section": "4.4 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "4.4 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\nThe basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\n\nshow the code\nwh <- wh %>%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\nRotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\n\nshow the code\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\nChanging the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\n\nshow the code\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nParallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\n\nshow the code\nhistoVisibility <- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-4",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started-4",
    "title": "Hands-on_Ex06",
    "section": "5.1 Getting Started",
    "text": "5.1 Getting Started\n\nInstall and launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\n\nshow the code\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-4",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#data-preparation-4",
    "title": "Hands-on_Ex06",
    "section": "5.2 Data Preparation",
    "text": "5.2 Data Preparation\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\n\nshow the code\nrealis2018 <- read_csv(\"data/realis2018.csv\")\n\n\n\nData Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and compute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively. Two key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained. grouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables. mutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”). sample_n() and sample_frac() sample the specified number/fraction of rows in each group. summarise() computes the summary for each group. In our case, group_by() will used together with summarise() to derive the summarised data.frame.\nGrouped summaries without the Pipe\n\n\nshow the code\nrealis2018_grouped <- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised <- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\nGrouped summaries with the pipe The code chunk below shows a more efficient way to tackle the same processes by using the pipe, %>%:\n\n\nshow the code\nrealis2018_summarised <- realis2018 %>% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %>%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-treemap-with-treemap-package",
    "title": "Hands-on_Ex06",
    "section": "5.3 Designing Treemap with treemap Package",
    "text": "5.3 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\nDesigning a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\n\nshow the code\nrealis2018_selected <- realis2018_summarised %>%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nUsing the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used:\nindex The index vector must consist of at least two column names or else no hierarchy treemap will be plotted. If multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on. vSize The column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps. Warning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nWorking with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThinking to learn from the conde chunk above.\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices. The legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\nColours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\nThe “value” type treemap\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThing to learn from the code chunk above:\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive. The reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\nThe “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above:\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative To overcome this problem, a single colour palette such as Blues should be used.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\nWorking with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nUsing sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\n\nshow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on_Ex06",
    "section": "5.4 Designing Treemap using treemapify Package",
    "text": "5.4 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\nDesigning a basic treemap\n\n\nshow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nDefining hierarchy\n\n\nshow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\nshow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\nshow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on_Ex06",
    "section": "5.5 Designing Interactive Treemap using d3treeR",
    "text": "5.5 Designing Interactive Treemap using d3treeR\n\nInstalling d3treeR package\nThis slide shows you how to install a R package which is not available in cran.\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\nshow the code\n#install.packages(\"devtools\")\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nshow the code\n#library(devtools)\n#install_github(\"timelyportfolio/d3treeR\")\n\n\nNow you are ready to launch d3treeR package\n\n\nshow the code\n#library(d3treeR)\n\n\n\n\nDesigning An Interactive Treemap\nThe codes below perform two processes.\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\nshow the code\n#tm <- treemap(realis2018_summarised,\n#        index=c(\"Planning Region\", \"Planning Area\"),\n#        vSize=\"Total Unit Sold\",\n#        vColor=\"Median Unit Price ($ psm)\",\n#        type=\"value\",\n#        title=\"Private Residential Property Sold, 2017\",\n#        title.legend = \"Median Unit Price (S$ per sq. m)\"\n#        )\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nshow the code\n#d3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if packages are installed in the computer. If they are, then they will be launched into R. The R packages installed are: plotly and tidyverse\n\n\nshow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\nshow the code\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n\nFor example,kable() can be used to review the structure of the imported data frame.\n\n\nshow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\ntimestamp field stores date-time values in POSIXct format. source_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code. tz field stores time zone of the source IP address.\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\n\nshow the code\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\nymd_hms() and hour() are from lubridate package, and weekdays() is a base R function.\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nshow the code\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\n\nshow the code\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\n\n\nshow the code\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\ncStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\ncount the number of attacks by country, calculate the percent of attackes by country, and save the results in a tibble data frame.\n\n\nshow the code\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\n\n\n\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\n\nshow the code\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()\n\n\n\n\n\n\n\nshow the code\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nshow the code\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\n\nshow the code\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\n\nshow the code\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\n\nshow the code\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\n\n\nshow the code\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")\n\n\n\n\n\n\n\n\n\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\n\nshow the code\nrice <- read_csv(\"data/rice.csv\")\n\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\n\nshow the code\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#getting-started-1",
    "title": "Hands-on_Ex07",
    "section": "2.1 Getting started",
    "text": "2.1 Getting started\nBefore getting start, make sure that ggHoriPlot has been included in the pacman::p_load(…) statement above.\n\n\nshow the code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-preparation-2",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#data-preparation-2",
    "title": "Hands-on_Ex07",
    "section": "2.2 Data Preparation",
    "text": "2.2 Data Preparation\nFor the purpose of this hands-on exercise, Average Retail Prices Of Selected Consumer Items will be used.\n\nStep 1: Data Import\nUse the code chunk below to import the AVERP.csv file into R environment.\n\n\nshow the code\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\nStep 2: Plotting the horizon graph\nNext, the code chunk below will be used to plot the horizon graph.\n\n\nshow the code\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  }
]